{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Transformer for Summary Generation","metadata":{}},{"cell_type":"markdown","source":"<a id='section01'></a>\n### Preparing Environment and Importing Libraries\n\nAt this step we will be installing the necessary libraries followed by importing the libraries and modules needed to run our script. \nWe will be installing:\n* transformers\n* wandb\n\nLibraries imported are:\n* Pandas\n* Pytorch\n* Pytorch Utils for Dataset and Dataloader\n* Transformers\n* T5 Model and Tokenizer\n\nFollowed by that we will preapre the device for CUDA execeution. This configuration is needed if you want to leverage on onboard GPU. First we will check the GPU avaiable to us, using the nvidia command followed by defining our device.","metadata":{}},{"cell_type":"code","source":"!pip install transformers -q\n\n# Code for TPU packages install\n# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"id":"WD_vnyLXZQzD","outputId":"b2ff57b8-a147-4893-80bd-e40d18042f98","execution":{"iopub.status.busy":"2023-06-07T14:52:00.146480Z","iopub.execute_input":"2023-06-07T14:52:00.146842Z","iopub.status.idle":"2023-06-07T14:52:07.632184Z","shell.execute_reply.started":"2023-06-07T14:52:00.146806Z","shell.execute_reply":"2023-06-07T14:52:07.631142Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: You are using pip version 20.1; however, version 23.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing stock libraries\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\n# Importing the T5 modules from huggingface/transformers\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration","metadata":{"id":"pzM1_ykHaFur","outputId":"58fa0ba8-b486-4b26-aaea-c0331b343b70","execution":{"iopub.status.busy":"2023-06-07T14:52:07.634298Z","iopub.execute_input":"2023-06-07T14:52:07.634606Z","iopub.status.idle":"2023-06-07T14:52:13.941246Z","shell.execute_reply.started":"2023-06-07T14:52:07.634575Z","shell.execute_reply":"2023-06-07T14:52:13.940255Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Checking out the GPU we have access to. This is output is from the google colab version. \n!nvidia-smi","metadata":{"id":"KvPxXdKJguYB","outputId":"6c523635-a25a-429b-cbd8-7b8bf9636972","execution":{"iopub.status.busy":"2023-06-07T14:52:13.942953Z","iopub.execute_input":"2023-06-07T14:52:13.943347Z","iopub.status.idle":"2023-06-07T14:52:14.928942Z","shell.execute_reply.started":"2023-06-07T14:52:13.943297Z","shell.execute_reply":"2023-06-07T14:52:14.928051Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Wed Jun  7 14:52:14 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Setting up the device for GPU usage\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\n\n# Preparing for TPU usage\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()","metadata":{"id":"NLxxwd1scQNv","execution":{"iopub.status.busy":"2023-06-07T14:52:14.931690Z","iopub.execute_input":"2023-06-07T14:52:14.932116Z","iopub.status.idle":"2023-06-07T14:52:14.959010Z","shell.execute_reply.started":"2023-06-07T14:52:14.932072Z","shell.execute_reply":"2023-06-07T14:52:14.957712Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a id='section02'></a>\n### Preparing the Dataset for data processing: Class\n\nWe will start with creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. This dataset will be used the the Dataloader method that will feed  the data in batches to the neural network for suitable training and processing. \nThe Dataloader and Dataset will be used inside the `main()`.\nDataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n\n#### *CustomDataset* Dataset Class\n- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the **T5** model for training. \n- We are using the **T5** tokenizer to tokenize the data in the `text` and `ctext` column of the dataframe. \n- The tokenizer uses the ` batch_encode_plus` method to perform tokenization and generate the necessary outputs, namely: `source_id`, `source_mask` from the actual text and `target_id` and `target_mask` from the summary text.\n- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer)\n- The *CustomDataset* class is used to create 2 datasets, for training and for validation.\n- *Training Dataset* is used to fine tune the model: **80% of the original data**\n- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n\n#### Dataloader: Called inside the `main()`\n- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of data loaded to the memory and then passed to the neural network needs to be controlled.\n- This control is achieved using the parameters such as `batch_size` and `max_len`.\n- Training and Validation dataloaders are used in the training and validation part of the flow respectively","metadata":{}},{"cell_type":"code","source":"# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.source_len = 300 # original: source_len\n        self.summ_len = summ_len\n        self.text = self.data.text\n        self.ctext = self.data.ctext\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        ctext = str(self.ctext[index])\n        ctext = ' '.join(ctext.split())\n\n        text = str(self.text[index])\n        text = ' '.join(text.split())\n\n        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"id":"932p8NhxeNw4","execution":{"iopub.status.busy":"2023-06-07T14:57:08.689596Z","iopub.execute_input":"2023-06-07T14:57:08.689984Z","iopub.status.idle":"2023-06-07T14:57:08.703326Z","shell.execute_reply.started":"2023-06-07T14:57:08.689940Z","shell.execute_reply":"2023-06-07T14:57:08.702306Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<a id='section03'></a>\n### Fine Tuning the Model: Function\n\nHere we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n\nThis function is called in the `main()`\n\nFollowing events happen in this function to fine tune the neural network:\n- The epoch, tokenizer, model, device details, testing_ dataloader and optimizer are passed to the `train ()` when its called from the `main()`\n- The dataloader passes data to the model based on the batch size.\n- `language_model_labels` are calculated from the `target_ids` also, `source_id` and `attention_mask` are extracted.\n- The model outputs first element gives the loss for the forward pass. \n- Loss value is used to optimize the weights of the neurons in the network.\n- After every 10 steps the loss value is logged in the wandb service. This log is then used to generate graphs for analysis. Such as [these](https://app.wandb.ai/abhimishra-91/transformers_tutorials_summarization?workspace=user-abhimishra-91)\n- After every 500 steps the loss value is printed in the console.","metadata":{}},{"cell_type":"code","source":"# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \ndef train(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    print('||| Training Model |||')\n    for _,data in tqdm(enumerate(loader, 0)):\n        y = data['target_ids'].to(device, dtype = torch.long)\n        y_ids = y[:, :-1].contiguous()\n        lm_labels = y[:, 1:].clone().detach()\n        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n        ids = data['source_ids'].to(device, dtype = torch.long)\n        mask = data['source_mask'].to(device, dtype = torch.long)\n\n        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n        loss = outputs[0]\n        \n        if _%500==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # xm.optimizer_step(optimizer)\n        # xm.mark_step()","metadata":{"id":"SaPAR7TWmxoM","execution":{"iopub.status.busy":"2023-06-07T14:58:52.313774Z","iopub.execute_input":"2023-06-07T14:58:52.314180Z","iopub.status.idle":"2023-06-07T14:58:52.325557Z","shell.execute_reply.started":"2023-06-07T14:58:52.314145Z","shell.execute_reply":"2023-06-07T14:58:52.324531Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<a id='section04'></a>\n### Validating the Model Performance: Function\n\nDuring the validation stage we pass the unseen data(Testing Dataset), trained model, tokenizer and device details to the function to perform the validation run. This step generates new summary for dataset that it has not seen during the training session. \n\nThis function is called in the `main()`\n\nThis unseen data is the 20% of `news_summary.csv` which was seperated during the Dataset creation stage. \nDuring the validation stage the weights of the model are not updated. We use the generate method for generating new text for the summary. \n\nIt depends on the `Beam-Search coding` method developed for sequence generation for models with LM head. \n\nThe generated text and originally summary are decoded from tokens to text and returned to the `main()`","metadata":{}},{"cell_type":"code","source":"def validate(epoch, tokenizer, model, device, loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    print('||| Model Validation |||')\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(loader, 0)):\n            y = data['target_ids'].to(device, dtype = torch.long)\n            ids = data['source_ids'].to(device, dtype = torch.long)\n            mask = data['source_mask'].to(device, dtype = torch.long)\n\n            generated_ids = model.generate(\n                input_ids = ids,\n                attention_mask = mask, \n                max_length=100, \n                num_beams=2,\n                temperature=1.0,\n                repetition_penalty=2.5, \n                length_penalty=1.2, \n                early_stopping=True\n            )\n            \n            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n            \n            if _%100==0:\n                print(f'Completed {_}')\n\n            predictions.extend(preds)\n            actuals.extend(target)\n    return predictions, actuals","metadata":{"id":"j9TNdHlQ0CLz","execution":{"iopub.status.busy":"2023-06-07T15:00:39.492786Z","iopub.execute_input":"2023-06-07T15:00:39.493209Z","iopub.status.idle":"2023-06-07T15:00:39.504903Z","shell.execute_reply.started":"2023-06-07T15:00:39.493172Z","shell.execute_reply":"2023-06-07T15:00:39.504039Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<a id='section05'></a>\n### Main Function\n\nThe `main()` as the name suggests is the central location to execute all the functions/flows created above in the notebook. The following steps are executed in the `main()`:\n\n\n<a id='section502'></a>\n#### Importing and Pre-Processing the domain data\n\nWe will be working with the data and preparing it for fine tuning purposes. \n*Assuming that the `news_summary.csv` is already downloaded in your `data` folder*\n\n* The file is imported as a dataframe and give it the headers as per the documentation.\n* Cleaning the file to remove the unwanted columns.\n* A new string is added to the main article column `summarize: ` prior to the actual article. This is done because **T5** had similar formatting for the summarization dataset. \n* The final Dataframe will be something like this:\n\n|text|ctext|\n|--|--|\n|summary-1|summarize: article 1|\n|summary-2|summarize: article 2|\n|summary-3|summarize: article 3|\n\n* Top 5 rows of the dataframe are printed on the console.\n\n<a id='section503'></a>\n#### Creation of Dataset and Dataloader\n\n* The updated dataframe is divided into 80-20 ratio for test and validation. \n* Both the data-frames are passed to the `CustomerDataset` class for tokenization of the new articles and their summaries.\n* The tokenization is done using the length parameters passed to the class.\n* Train and Validation parameters are defined and passed to the `pytorch Dataloader contstruct` to create `train` and `validation` data loaders.\n* These dataloaders will be passed to `train()` and `validate()` respectively for training and validation action.\n* The shape of datasets is printed in the console.\n\n\n<a id='section504'></a>\n#### Neural Network and Optimizer\n\n* In this stage we define the model and optimizer that will be used for training and to update the weights of the network. \n* We are using the `t5-base` transformer model for our project. You can read about the `T5 model` and its features above. \n* We use the `T5ForConditionalGeneration.from_pretrained(\"t5-base\")` commad to define our model. The `T5ForConditionalGeneration` adds a Language Model head to our `T5 model`. The Language Model head allows us to generate text based on the training of `T5 model`.\n* We are using the `Adam` optimizer for our project. This has been a standard for all our tutorials and is something that can be changed updated to see how different optimizer perform with different learning rates. \n* There is also a scope for doing more with Optimizer such a decay, momentum to dynamically update the Learning rate and other parameters. All those concepts have been kept out of scope for these tutorials. \n\n\n<a id='section505'></a>\n#### Training Model\n\n* We call the `train()` with all the necessary parameters.\n* Loss at every 500th step is printed on the console.\n\n\n<a id='section506'></a>\n#### Validation and generation of Summary\n\n* After the training is completed, the validation step is initiated.\n* As defined in the validation function, the model weights are not updated. We use the fine tuned model to generate new summaries based on the article text.\n* An output is printed on the console giving a count of how many steps are complete after every 100th step. \n* The original summary and generated summary are converted into a list and returned to the main function. \n* Both the lists are used to create the final dataframe with 2 columns **Generated Summary** and **Actual Summary**\n* The dataframe is saved as a csv file in the local drive.\n* A qualitative analysis can be done with the Dataframe. ","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', 'latin-1', engine='python')\n# df.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(df.loc[1]['author,date,headlines,read_more,text,ctext'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n\n# # Example loop\n# for i in tqdm(range(200)):\n#     # Perform some task\n#     time.sleep(0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    TRAIN_BATCH_SIZE = 4    # input batch size for training (default: 64)\n    VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n    TRAIN_EPOCHS = 4        # number of epochs to train (default: 10)\n    VAL_EPOCHS = 2\n    LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n    SEED = 42               # random seed (default: 42)\n    MAX_LEN = 300\n    SUMMARY_LEN = 100\n    T5MODEL = \"t5-small\"\n\n    # Set random seeds and deterministic pytorch for reproducibility\n    torch.manual_seed(SEED) # pytorch random seed\n    np.random.seed(SEED) # numpy random seed\n    torch.backends.cudnn.deterministic = True\n\n    # tokenzier for encoding the text\n    tokenizer = T5Tokenizer.from_pretrained(T5MODEL)\n\n    # Importing and Pre-Processing the domain data\n    # Selecting the needed columns only. \n    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n    df = pd.read_csv('../input/news-summary/news_summary.csv', encoding='latin-1')\n    df = df[['text','ctext']]\n    df.ctext = 'summarize: ' + df.ctext # specify task for t5 transformer (summarization)\n    print(df.sample(5))\n    \n    # Creation of Dataset and Dataloader\n    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n    train_size = 0.8\n    train_dataset=df.sample(frac=train_size, random_state = SEED).reset_index(drop=True)\n    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n\n    print(\"FULL Dataset: {}\".format(df.shape))\n    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n\n    # Creating the Training and Validation dataset for further creation of Dataloader\n    training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n\n    # Defining the parameters for creation of dataloaders\n    train_params = {\n        'batch_size': TRAIN_BATCH_SIZE,\n        'shuffle': True,\n        'num_workers': 0\n    }\n\n    val_params = {\n        'batch_size': VALID_BATCH_SIZE,\n        'shuffle': False,\n        'num_workers': 0\n    }\n\n    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n    training_loader = DataLoader(training_set, **train_params)\n    val_loader = DataLoader(val_set, **val_params)\n\n    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n    # Further this model is sent to device (GPU/TPU) for using the hardware.\n    model = T5ForConditionalGeneration.from_pretrained(T5MODEL)\n    model = model.to(device)\n\n    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n    optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n\n    # Training loop\n    # takes approx 4 mins per epoch, 452 iterations (batch size 8)\n    print('||| Initiating Fine-Tuning |||')\n    for epoch in tqdm(range(TRAIN_EPOCHS)):\n        train(epoch, tokenizer, model, device, training_loader, optimizer)\n\n    # takes approx 5 mins per 100 iterations \n    # Validation loop and saving the resulting file with predictions and actuals in a dataframe.\n    # Saving the dataframe as predictions.csv\n    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n    for epoch in tqdm(range(VAL_EPOCHS)):\n        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n        final_df.to_csv('predictions.csv')\n        print('Output Files generated for review')\n        \n    ######################################\n    # add code to save the model here\n#     torch.save(NewsSummaryModel().state_dict(), 'NewsSummaryModel.pth')\n    # Save the trained model and tokenizer\n    print('Saving Model, Tokenizer, Weights')\n    basepath = '/kaggle/working'\n    model.save_pretrained(basepath)\n    tokenizer.save_pretrained(basepath)\n#     model.save_weights(basepath)\n\nif __name__ == '__main__':\n    main()","metadata":{"id":"ZtNs9ytpCow2","outputId":"80545587-0a82-455a-a9ba-13eb3fcb1550","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# T5MODEL = \"t5-small\"\n# tokenizer = T5Tokenizer.from_pretrained(T5MODEL)\n# model = T5ForConditionalGeneration.from_pretrained(T5MODEL)\n# model = model.to(device)\n\n# basepath = '/kaggle/working'\n# model.save_pretrained(basepath)\n# tokenizer.save_pretrained(basepath)\n# # model.save_weights(basepath)","metadata":{"execution":{"iopub.status.busy":"2023-06-06T08:21:17.763312Z","iopub.execute_input":"2023-06-06T08:21:17.763718Z","iopub.status.idle":"2023-06-06T08:21:20.789676Z","shell.execute_reply.started":"2023-06-06T08:21:17.763685Z","shell.execute_reply":"2023-06-06T08:21:20.788452Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/spiece.model',\n '/kaggle/working/special_tokens_map.json',\n '/kaggle/working/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"pd.set_option(\"display.max_colwidth\", 200)\npreds = pd.read_csv('/kaggle/working/predictions.csv')\npreds.sample(10)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T15:51:02.404218Z","iopub.execute_input":"2023-06-07T15:51:02.404602Z","iopub.status.idle":"2023-06-07T15:51:02.435992Z","shell.execute_reply.started":"2023-06-07T15:51:02.404567Z","shell.execute_reply":"2023-06-07T15:51:02.435202Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0  \\\n323         323   \n32           32   \n435         435   \n863         863   \n247         247   \n702         702   \n844         844   \n129         129   \n381         381   \n473         473   \n\n                                                                                                                                                                                              Generated Text  \\\n323  State Bank of India has ruled out a spike in bad loans following the mega merger that catampulted the country's largest lender into top 50 globally with close to USD 500 billion balancesheet. \"Inc...   \n32   <extra_id_0>anas, he said. He added that the government has been able to provide support and support for the development of new technologies in the country. \"It'll be very important,\" he added on ...   \n435  the central government has allocated 965 Megahertz spectrum through auction in October, 2016 to various telecom service providers for access services. This will enable the telecom service provider...   \n863  Saina Nehwal, who was appointed 'Athletes Commission' last year to represent the panel in Badminton World Federation (BWF), would join the Bwf Athlete Commission as representative of IOC AC. She w...   \n247  Sunita Kaushik, who was handpicked by the party' CM Manoj Tiwari in an affildvit that she has assets worth 1.35 crore including three residential houses in west Delhi. Kaushi is not an income-tax ...   \n702  former Pakistan leg spinner Danish Kaneria has appealed to the cricket authorities to have his case revisiting by an inquiry tribunal set-up for probe into allegations against Sharjeels Khan and K...   \n844  the president of Poland's Supreme Court has urged judges to fight for every inch in justice as the rightwing government pushed for changes that criticis say would make judicial independence a \"pur...   \n129  Delhi metro has ordered 516 coaches (86 trains) to run only on these two lines and cannot be integrated with other existing lines. The trains will run only between Mumbai-Kalkaji Mandir and Botani...   \n381  Congress general secretary Rahul Gandhi on Tuesday met Tamil Nadu farmers protesting at Delhi's Jantar Mantar for over two weeks. \"Neither the government nor PM Modi listen to them (Tamil Nadu far...   \n473  students of Sharda Vidyan Mandir in Porbandar, Gujarat were caught in obscene videos after the teacher showed them obscene videos and passed lews comments. Not only did she dance and talked in an ...   \n\n                                                                                                                                                                                                 Actual Text  \n323  State Bank of India Chairperson Arundhati Bhattacharya has said that after the merger with associate banks, the lender has completed a \"mass mission\". \"I don't think anybody in the world would hav...  \n32   India has become the world's fifth-largest military spender, witnessing a growth of around 8.5% in expenditure in 2016, a report has claimed. According to the figures released by the Stockholm Int...  \n435  Referring to National Association of Software and Services Companies (NASSCOM) and Akamai, the Telecom Ministry on Monday said that Internet users in India will reach 730 million by 2020. Minister...  \n863  Indian shuttler Saina Nehwal, who became a member of the International Olympic Committee's Athletes' Commission (IOC AC) last year, would be representing the panel in the Badminton World Federatio...  \n247  A BJP candidate for the upcoming MCD polls in Delhi Sunita Kaushik, touted as a slum dweller and the face of the city's urban poor, turned out to be a crorepati. In her election affidavit, Kaushik...  \n702  Former Pakistan leg-spinner Danish Kaneria, serving a life ban for spot-fixing, has appealed to the cricket authorities to have his case revisited by the inquiry tribunal set-up to probe into the ...  \n844  The President of Poland's Supreme Court Malgorzata Gersdorf has urged the country's judges to \"fight every inch\" for justice as the ruling party plans to \"democratise\" the way judges are appointed...  \n129  The Delhi Metro Rail Corporation (DMRC) is set to get its first 'driverless' trains on tracks between Noida and Kalkaji in June this year. These trains will run only on two metro lines? Pink (Muku...  \n381  Congress Vice-President Rahul Gandhi on Friday met Tamil Nadu farmers protesting at Delhi's Jantar Mantar for over two weeks and accused Prime Minister Narendra Modi of neglecting the farmers whil...  \n473  A female teacher from Sharda Vidya Mandir in Porbandar, Gujarat, allegedly showed obscene videos to students and danced half-naked in front of them after taking them inside a room. While she threa...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Generated Text</th>\n      <th>Actual Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>323</th>\n      <td>323</td>\n      <td>State Bank of India has ruled out a spike in bad loans following the mega merger that catampulted the country's largest lender into top 50 globally with close to USD 500 billion balancesheet. \"Inc...</td>\n      <td>State Bank of India Chairperson Arundhati Bhattacharya has said that after the merger with associate banks, the lender has completed a \"mass mission\". \"I don't think anybody in the world would hav...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>&lt;extra_id_0&gt;anas, he said. He added that the government has been able to provide support and support for the development of new technologies in the country. \"It'll be very important,\" he added on ...</td>\n      <td>India has become the world's fifth-largest military spender, witnessing a growth of around 8.5% in expenditure in 2016, a report has claimed. According to the figures released by the Stockholm Int...</td>\n    </tr>\n    <tr>\n      <th>435</th>\n      <td>435</td>\n      <td>the central government has allocated 965 Megahertz spectrum through auction in October, 2016 to various telecom service providers for access services. This will enable the telecom service provider...</td>\n      <td>Referring to National Association of Software and Services Companies (NASSCOM) and Akamai, the Telecom Ministry on Monday said that Internet users in India will reach 730 million by 2020. Minister...</td>\n    </tr>\n    <tr>\n      <th>863</th>\n      <td>863</td>\n      <td>Saina Nehwal, who was appointed 'Athletes Commission' last year to represent the panel in Badminton World Federation (BWF), would join the Bwf Athlete Commission as representative of IOC AC. She w...</td>\n      <td>Indian shuttler Saina Nehwal, who became a member of the International Olympic Committee's Athletes' Commission (IOC AC) last year, would be representing the panel in the Badminton World Federatio...</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>247</td>\n      <td>Sunita Kaushik, who was handpicked by the party' CM Manoj Tiwari in an affildvit that she has assets worth 1.35 crore including three residential houses in west Delhi. Kaushi is not an income-tax ...</td>\n      <td>A BJP candidate for the upcoming MCD polls in Delhi Sunita Kaushik, touted as a slum dweller and the face of the city's urban poor, turned out to be a crorepati. In her election affidavit, Kaushik...</td>\n    </tr>\n    <tr>\n      <th>702</th>\n      <td>702</td>\n      <td>former Pakistan leg spinner Danish Kaneria has appealed to the cricket authorities to have his case revisiting by an inquiry tribunal set-up for probe into allegations against Sharjeels Khan and K...</td>\n      <td>Former Pakistan leg-spinner Danish Kaneria, serving a life ban for spot-fixing, has appealed to the cricket authorities to have his case revisited by the inquiry tribunal set-up to probe into the ...</td>\n    </tr>\n    <tr>\n      <th>844</th>\n      <td>844</td>\n      <td>the president of Poland's Supreme Court has urged judges to fight for every inch in justice as the rightwing government pushed for changes that criticis say would make judicial independence a \"pur...</td>\n      <td>The President of Poland's Supreme Court Malgorzata Gersdorf has urged the country's judges to \"fight every inch\" for justice as the ruling party plans to \"democratise\" the way judges are appointed...</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>129</td>\n      <td>Delhi metro has ordered 516 coaches (86 trains) to run only on these two lines and cannot be integrated with other existing lines. The trains will run only between Mumbai-Kalkaji Mandir and Botani...</td>\n      <td>The Delhi Metro Rail Corporation (DMRC) is set to get its first 'driverless' trains on tracks between Noida and Kalkaji in June this year. These trains will run only on two metro lines? Pink (Muku...</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>381</td>\n      <td>Congress general secretary Rahul Gandhi on Tuesday met Tamil Nadu farmers protesting at Delhi's Jantar Mantar for over two weeks. \"Neither the government nor PM Modi listen to them (Tamil Nadu far...</td>\n      <td>Congress Vice-President Rahul Gandhi on Friday met Tamil Nadu farmers protesting at Delhi's Jantar Mantar for over two weeks and accused Prime Minister Narendra Modi of neglecting the farmers whil...</td>\n    </tr>\n    <tr>\n      <th>473</th>\n      <td>473</td>\n      <td>students of Sharda Vidyan Mandir in Porbandar, Gujarat were caught in obscene videos after the teacher showed them obscene videos and passed lews comments. Not only did she dance and talked in an ...</td>\n      <td>A female teacher from Sharda Vidya Mandir in Porbandar, Gujarat, allegedly showed obscene videos to students and danced half-naked in front of them after taking them inside a room. While she threa...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Saving Data","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom IPython.display import FileLink\n\ndef zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n    \"\"\"\n    zip all the files in a directory\n    \n    Parameters\n    _____\n    directory: str\n        directory needs to be zipped, defualt is current working directory\n        \n    file_name: str\n        the name of the zipped file (including .zip), default is 'directory.zip'\n        \n    Returns\n    _____\n    Creates a hyperlink, which can be used to download the zip file)\n    \"\"\"\n    os.chdir(directory)\n    zip_ref = zipfile.ZipFile(file_name, mode='w')\n    for folder, _, files in os.walk(directory):\n        for file in files:\n            if file_name in file:\n                pass\n            else:\n                zip_ref.write(os.path.join(folder, file))\n\n    return FileLink(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T15:51:06.956778Z","iopub.execute_input":"2023-06-07T15:51:06.957167Z","iopub.status.idle":"2023-06-07T15:51:06.965924Z","shell.execute_reply.started":"2023-06-07T15:51:06.957132Z","shell.execute_reply":"2023-06-07T15:51:06.964975Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"zip_dir()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T15:51:07.329038Z","iopub.execute_input":"2023-06-07T15:51:07.329402Z","iopub.status.idle":"2023-06-07T15:51:08.023542Z","shell.execute_reply.started":"2023-06-07T15:51:07.329368Z","shell.execute_reply":"2023-06-07T15:51:08.022611Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/directory.zip","text/html":"<a href='directory.zip' target='_blank'>directory.zip</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Loading and Using Model","metadata":{}},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration, T5Tokenizer\n\n# Load the saved model and tokenizer from a specific path\nload_directory = '/kaggle/working/'\nmodel = T5ForConditionalGeneration.from_pretrained(load_directory)\ntokenizer = T5Tokenizer.from_pretrained(load_directory)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T06:17:46.830868Z","iopub.execute_input":"2023-06-05T06:17:46.831256Z","iopub.status.idle":"2023-06-05T06:17:48.673748Z","shell.execute_reply.started":"2023-06-05T06:17:46.831221Z","shell.execute_reply":"2023-06-05T06:17:48.672916Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Example input text\ninput_text = \"\"\n\n# Tokenize the input text\ninput_ids = tokenizer.encode(input_text, truncation=True, padding='longest', return_tensors='pt')\n\n# Generate the summary\nsummary_ids = model.generate(input_ids, max_length=2048, num_beams=2, early_stopping=True)\nsummary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nprint(\"||| Input text |||\\n\", input_text)\nprint(\"\\n||| Generated summary ||| \\n\", summary_text)","metadata":{"execution":{"iopub.status.busy":"2023-06-05T06:31:25.708216Z","iopub.execute_input":"2023-06-05T06:31:25.708586Z","iopub.status.idle":"2023-06-05T06:31:29.484219Z","shell.execute_reply.started":"2023-06-05T06:31:25.708551Z","shell.execute_reply":"2023-06-05T06:31:29.483207Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"||| Input text |||\n After much wait, the first UDAN flight took off from Shimla today after being flagged off by Prime Minister Narendra Modi.The flight will be operated by Alliance Air, the regional arm of Air India. PM Narendra Modi handed over boarding passes to some of passengers travelling via the first UDAN flight at the Shimla airport.Tomorrow PM @narendramodi will flag off the first UDAN flight under the Regional Connectivity Scheme, on Shimla-Delhi sector.Air India yesterday opened bookings for the first launch flight from Shimla to Delhi with all inclusive fares starting at Rs2,036.THE GREAT 'UDAN'The UDAN (Ude Desh ka Aam Naagrik) scheme seeks to make flying more affordable for the common people, holding a plan to connect over 45 unserved and under-served airports.Under UDAN, 50 per cent of the seats on each flight would have a cap of Rs 2,500 per seat/hour. The government has also extended subsidy in the form of viability gap funding to the operators flying on these routes.The scheme was launched to 'make air travel accessible to citizens in regionally important cities,' and has been described as 'a first-of-its-kind scheme globally to stimulate regional connectivity through a market-based mechanism.' Report have it the first flight today will not be flying at full capacity on its 70-seater ATR airplane because of payload restrictions related to the short Shimla airfield.|| Read more ||Udan scheme: Now you can fly to these 43 cities, see the full list hereUDAN scheme to fly hour-long flights capped at Rs 2,500 to smaller cities\n\n||| Generated summary ||| \n <extra_id_0>?2,500 to smaller cities. The first UDAN flight took off from Shimla today after being flagged off by Prime Minister Narendra Modi. The UDAN scheme to fly hour-long flights capped at?2,500 to smaller cities. The scheme seeks to make air travel accessible to citizens in regionally important cities, holding a plan to connect over 45 unserved and under-served airports. The scheme seeks to make flying more affordable for common people, holding a plan to connect over 45\n","output_type":"stream"}]}]}